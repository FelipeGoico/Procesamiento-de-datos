import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
import networkx as nx
import plotly.graph_objects as go
import plotly.express as px
from sklearn.decomposition import PCA


df = st.session_state.df


# ===========================
# Estilo CSS personalizado
# ===========================
st.markdown("""
<style>
table {
    width: 100%;
    border-collapse: collapse;
    margin: 12px 0;
    font-size: 15px;
    font-family: 'Segoe UI', sans-serif;
}
th {
    background-color: #1f4e79;
    color: white;
    text-align: center;
    padding: 8px;
    border: 1px solid #ddd;
}
td {
    background-color: #f9f9f9;
    text-align: left;
    padding: 8px;
    border: 1px solid #ddd;
}
tr:nth-child(even) td {
    background-color: #f1f1f1;
}
</style>
""", unsafe_allow_html=True)


# sube desde pages -> app -> Procesamiento-de-datos
BASE_DIR = Path(__file__).resolve().parent.parent.parent
IMG_DIR = BASE_DIR / "img"
IMAGE_PATH_00 = IMG_DIR / "fvets-12-1630083-g000.jpg"


# ===========================
# Configuraci√≥n p√°gina
# ===========================
st.set_page_config(
    page_title="EDA - Dataset db-cow-walking-IoT",
    page_icon=":mag:",
    layout="wide"
)

# ===========================
# Banner
# ===========================
st.markdown("""
<div style="background-color:#1f4e79; padding: 18px; border-radius: 10px; text-align:center; color: white;">
    <h1 style="margin:0;">üîç An√°lisis Exploratorio de Datos (EDA)</h1>
    <p style="margin:0; font-size:18px;">Dataset <i>db-cow-walking-IoT</i></p>
</div>
""", unsafe_allow_html=True)


# ===========================
# Introducci√≥n con continuidad
# ===========================
st.markdown("""
<div style="text-align: justify; margin-top: 20px;">
El presente An√°lisis Exploratorio de Datos (EDA) se basa en el dataset original 
<b>db-cow-walking-IoT</b>, que recoge informaci√≥n de sensores inerciales (IMU) y GPS en vacas lecheras de pastoreo.  
Este conjunto de datos ha permitido identificar y clasificar comportamientos como caminar, pastorear o descansar.  

Dando continuidad al proyecto anterior, utilizaremos este EDA como <b>punto de partida</b> para profundizar en nuevas l√≠neas de investigaci√≥n.  
Nuestro objetivo es ampliar la detecci√≥n hacia <b>comportamientos an√≥malos</b>, como espasmos o sacudidas vinculadas a la presencia de moscas, en especial en zonas del lomo y los cuernos.  
La incorporaci√≥n de estas nuevas variables derivadas no solo enriquecer√° el dataset, sino que tambi√©n abrir√° la posibilidad de <b>automatizar la detecci√≥n temprana</b> de posibles problemas de salud animal, mejorando as√≠ la gesti√≥n en la ganader√≠a moderna.  
</div>
""", unsafe_allow_html=True)


# ==========================================================
# ==================== Preparaci√≥n de Datos ================
# ==========================================================
st.markdown("""
## ‚öôÔ∏è Tratamiento y Preparaci√≥n de los Datos

<div style="text-align: justify;">
Los datos originales se encontraban organizados en m√∫ltiples carpetas, donde cada una representaba un comportamiento distinto 
(caminata, pastoreo, reposo, entre otros). Cada carpeta conten√≠a varios archivos <code>.csv</code> con registros capturados 
desde los sensores <b>MPU9250</b> y <b>BNO055</b>.  

Para unificar y estructurar esta informaci√≥n, se dise√±√≥ un <b>m√©todo adaptable</b> que permite:  

1. Recorrer autom√°ticamente todas las carpetas y archivos.  
2. Incorporar una columna <code>label</code> con el nombre de la carpeta (comportamiento observado).  
3. Calcular magnitudes derivadas de aceleraci√≥n y giroscopio.  
4. Generar estad√≠sticas de ventana temporal (rolling window) para capturar variaciones din√°micas.  
5. Crear indicadores de alerta como <i>inquietud alta</i> o <i>actividad extrema</i>.  

Este enfoque permiti√≥ transformar un conjunto de archivos dispersos en un dataset unificado, consistente y listo para el an√°lisis exploratorio.  
</div>
""", unsafe_allow_html=True)


# ===========================
# Mostrar c√≥digo usado (colapsable)
# ===========================
with st.expander("üìú Ver c√≥digo usado en el tratamiento de los datos"):
    st.code("""
import os
import pandas as pd 
import numpy as np

def load_data_from_folders(base_path):
    df_list = []
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                folder_name = os.path.basename(root)
                df = pd.read_csv(file_path)
                df['label'] = folder_name
                df_list.append(df)
    df_full = pd.concat(df_list, ignore_index=True)
    return df_full

df_full = load_data_from_folders('C:/ruta/base/db-cow-walking-IoT')
df = df_full.copy()

# Magnitudes de sensores
df['BNO055_acc_magnitude'] = np.sqrt(df['BNO055_AX']**2 + df['BNO055_AY']**2 + df['BNO055_AZ']**2)
df['BNO055_gyro_magnitude'] = np.sqrt(df['BNO055_GX']**2 + df['BNO055_GY']**2 + df['BNO055_GZ']**2)
df['MPU9250_acc_magnitude'] = np.sqrt(df['MPU9250_AX']**2 + df['MPU9250_AY']**2 + df['MPU9250_AZ']**2)
df['MPU9250_gyro_magnitude'] = np.sqrt(df['MPU9250_GX']**2 + df['MPU9250_GY']**2 + df['MPU9250_GZ']**2)

# Estad√≠sticas por ventana
window_size = 5
df['acc_mean_window5'] = df['BNO055_acc_magnitude'].rolling(window_size).mean()
df['acc_std_window5'] = df['BNO055_acc_magnitude'].rolling(window_size).std()
df['gyro_mean_window5'] = df['BNO055_gyro_magnitude'].rolling(window_size).mean()
df['gyro_std_window5'] = df['BNO055_gyro_magnitude'].rolling(window_size).std()

# Indicadores de alerta
df['inquietud_alta'] = (
    (df['label'] == 'Resting') & 
    (df['BNO055_gyro_magnitude'] > df['BNO055_gyro_magnitude'].quantile(0.95))
).astype(int)

df['actividad_extrema'] = (
    df['BNO055_acc_magnitude'] > df['BNO055_acc_magnitude'].quantile(0.95)
).astype(int)
    """, language="python")


st.image(str(IMAGE_PATH_00),
         caption="Estructura de carpetas y archivos del dataset original", use_container_width=True)

# ===========================
# Dataset y variables
# ===========================
st.markdown("""
## üìä Tipos de Variables en el Dataset
El dataset contiene tanto variables num√©ricas como categ√≥ricas que permiten representar el comportamiento de las vacas.
""")


# ===========================
# Secci√≥n descriptiva
# ===========================
st.markdown("## üìë Descripci√≥n de las Variables")
st.markdown("""
Esta secci√≥n presenta una descripci√≥n detallada de las variables contenidas en el dataset
**db-cow-walking-IoT**, clasificadas seg√∫n su origen y naturaleza.
Se busca mantener una presentaci√≥n clara, concisa y profesional para apoyar la investigaci√≥n.
""")


# ===========================
# Variables de Tiempo
# ===========================
with st.expander("üïí Variables de Tiempo", expanded=True):
    st.markdown("""
| Nombre | Descripci√≥n | Tipo de dato | Unidad |
| :--- | :--- | :--- | :--- |
| **Time** | Timestamp que indica el momento de registro de la muestra. | Num√©rica Continua | timestamp |
""")

# ===========================
# Variables del Sensor BNO055
# ===========================
with st.expander("üì° Variables del Sensor BNO055"):
    st.markdown("""
| Nombre | Descripci√≥n | Tipo de dato | Unidad |
| :--- | :--- | :--- | :--- |
| **BNO055_ARX** | Tasa de rotaci√≥n absoluta eje X. | Num√©rica Continua | ¬∞/s o rad/s |
| **BNO055_ARY** | Tasa de rotaci√≥n absoluta eje Y. | Num√©rica Continua | ¬∞/s o rad/s |
| **BNO055_ARZ** | Tasa de rotaci√≥n absoluta eje Z. | Num√©rica Continua | ¬∞/s o rad/s |
| **BNO055_AX** | Aceleraci√≥n lineal eje X. | Num√©rica Continua | m/s¬≤ |
| **BNO055_AY** | Aceleraci√≥n lineal eje Y. | Num√©rica Continua | m/s¬≤ |
| **BNO055_AZ** | Aceleraci√≥n lineal eje Z. | Num√©rica Continua | m/s¬≤ |
| **BNO055_GX** | Velocidad angular eje X. | Num√©rica Continua | ¬∞/s |
| **BNO055_GY** | Velocidad angular eje Y. | Num√©rica Continua | ¬∞/s |
| **BNO055_GZ** | Velocidad angular eje Z. | Num√©rica Continua | ¬∞/s |
| **BNO055_MX** | Campo magn√©tico eje X. | Num√©rica Continua | ¬µT |
| **BNO055_MY** | Campo magn√©tico eje Y. | Num√©rica Continua | ¬µT |
| **BNO055_MZ** | Campo magn√©tico eje Z. | Num√©rica Continua | ¬µT |
| **BNO055_Q0** | Componente escalar (w) del cuaterni√≥n. | Num√©rica Continua | ‚Äî |
| **BNO055_Q1** | Componente X (i) del cuaterni√≥n. | Num√©rica Continua | ‚Äî |
| **BNO055_Q2** | Componente Y (j) del cuaterni√≥n. | Num√©rica Continua | ‚Äî |
| **BNO055_Q3** | Componente Z (k) del cuaterni√≥n. | Num√©rica Continua | ‚Äî |
""")

# ===========================
# Variables del Sensor MPU9250
# ===========================
with st.expander("‚öôÔ∏è Variables del Sensor MPU9250"):
    st.markdown("""
| Nombre | Descripci√≥n | Tipo de dato | Unidad |
| :--- | :--- | :--- | :--- |
| **MPU9250_AX** | Aceleraci√≥n lineal eje X. | Num√©rica Continua | m/s¬≤ |
| **MPU9250_AY** | Aceleraci√≥n lineal eje Y. | Num√©rica Continua | m/s¬≤ |
| **MPU9250_AZ** | Aceleraci√≥n lineal eje Z. | Num√©rica Continua | m/s¬≤ |
| **MPU9250_GX** | Velocidad angular eje X. | Num√©rica Continua | ¬∞/s |
| **MPU9250_GY** | Velocidad angular eje Y. | Num√©rica Continua | ¬∞/s |
| **MPU9250_GZ** | Velocidad angular eje Z. | Num√©rica Continua | ¬∞/s |
| **MPU9250_MX** | Campo magn√©tico eje X. | Num√©rica Continua | ¬µT |
| **MPU9250_MY** | Campo magn√©tico eje Y. | Num√©rica Continua | ¬µT |
| **MPU9250_MZ** | Campo magn√©tico eje Z. | Num√©rica Continua | ¬µT |
""")

# ===========================
# Variables Derivadas
# ===========================
with st.expander("üßÆ Variables Derivadas"):
    st.markdown("""
| Nombre | Descripci√≥n | Tipo de dato | Unidad |
| :--- | :--- | :--- | :--- |
| **BNO055_acc_magnitude** | Magnitud de la aceleraci√≥n BNO055. | Num√©rica Continua | m/s¬≤ |
| **BNO055_gyro_magnitude** | Magnitud del giroscopio BNO055. | Num√©rica Continua | ¬∞/s |
| **MPU9250_acc_magnitude** | Magnitud de la aceleraci√≥n MPU9250. | Num√©rica Continua | m/s¬≤ |
| **MPU9250_gyro_magnitude** | Magnitud del giroscopio MPU9250. | Num√©rica Continua | ¬∞/s |
| **acc_mean_window5** | Media de aceleraci√≥n en ventana de 5. | Num√©rica Continua | m/s¬≤ |
| **acc_std_window5** | Desviaci√≥n est√°ndar de aceleraci√≥n en ventana de 5. | Num√©rica Continua | m/s¬≤ |
| **gyro_mean_window5** | Media del giroscopio en ventana de 5. | Num√©rica Continua | ¬∞/s |
| **gyro_std_window5** | Desviaci√≥n est√°ndar del giroscopio en ventana de 5. | Num√©rica Continua | ¬∞/s |
""")

# ===========================
# Variables de Etiqueta
# ===========================
with st.expander("üè∑Ô∏è Variables de Etiqueta"):
    st.markdown("""
| Nombre | Descripci√≥n | Tipo de dato | Unidad |
| :--- | :--- | :--- | :--- |
| **label** | Clase categ√≥rica del comportamiento observado. | Categ√≥rica | ‚Äî |
| **inquietud_alta** | Indicador binario de alta inquietud. | Categ√≥rica | 0 / 1 |
| **actividad_extrema** | Indicador binario de actividad extrema. | Categ√≥rica | 0 / 1 |
""")

# ===========================
# Validaci√≥n de NA / Null
# ===========================
st.markdown("## ‚ö†Ô∏è Validaci√≥n de valores faltantes (NA / Null)")

# Cantidad de NA por columna
na_count = df.isna().sum()
# Porcentaje de NA por columna
na_percent = (na_count / len(df)) * 100

na_df = pd.DataFrame({
    "Columna": df.columns,
    "NA_count": na_count,
    "NA_percent": na_percent.round(2)
}).sort_values(by="NA_percent", ascending=False)

st.dataframe(na_df, use_container_width=True)

# Mensaje de alerta si hay NA
threshold = 5  # porcentaje considerado cr√≠tico
if any(na_percent > threshold):
    st.warning(f"Hay columnas con m√°s del {threshold}% de valores faltantes. Considera imputar o limpiar antes del an√°lisis.")
else:
    st.success("No se detectan valores faltantes relevantes.")

 
st.markdown("""
### üìä Distribuci√≥n de Variables Num√©ricas

Antes de explorar las relaciones entre variables, es importante conocer la distribuci√≥n individual de cada columna num√©rica. 
Esto permite detectar sesgos, valores at√≠picos o rangos inesperados, y sirve como base para an√°lisis posteriores como correlaciones o grafo de variables.
""")

fig, col_numericas = st.session_state.graphs
st.pyplot(fig)

st.markdown("### üìä Mapa de Correlaci√≥n de Variables Num√©ricas por columna")

# Seleccionar variables num√©ricas
col_numericas = df.select_dtypes(
    include=[np.number]).columns.drop('label', errors='ignore')

# Calcular matriz de correlaci√≥n
corr_matrix = df[col_numericas].corr()

# Triangular inferior: ponemos NaN en la parte superior
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
corr_masked = corr_matrix.mask(mask)

# Crear heatmap interactivo con Plotly
fig = px.imshow(
    corr_masked,
    text_auto='.2f',       # mostrar valores en cada celda
    aspect="auto",
    color_continuous_scale='RdBu_r',
    origin='upper'
)

# Reducir tama√±o de texto dentro de los cuadros
# ajusta seg√∫n convenga, 10 es ~40% m√°s peque√±o que el default
fig.update_traces(textfont_size=10)

# Mejorar layout
fig.update_layout(
    title='Mapa de Correlaci√≥n de Variables Num√©ricas (Triangular Inferior)',
    xaxis_title='Variables',
    yaxis_title='Variables',
    xaxis_tickangle=-45,
    width=900,
    height=900
)

st.plotly_chart(fig, use_container_width=True)



# ==========================================================
# T√≠tulo y descripci√≥n
# ==========================================================
st.markdown("### üï∏Ô∏è Grafo de Correlaci√≥n de Variables")
st.markdown("""
<div style="text-align: justify; margin-top: 10px;">
Esta visualizaci√≥n presenta las correlaciones m√°s fuertes como una red, donde cada nodo es una variable. 
Una l√≠nea (arista) entre dos nodos indica una correlaci√≥n significativa (positiva o negativa). Esto reduce la 
saturaci√≥n visual y ayuda a identificar r√°pidamente las relaciones clave.
</div>
""", unsafe_allow_html=True)

# Leyenda clara separada
st.markdown("""
**Leyenda:**  
<b style="color:red;">Rojo:</b> Correlaci√≥n positiva fuerte  
<b style="color:blue;">Azul:</b> Correlaci√≥n negativa fuerte  
Solo se muestran correlaciones > 0.7 o < -0.7
""", unsafe_allow_html=True)

# ==========================================================
# Preparar matriz de correlaci√≥n y edges
# ==========================================================
col_numericas = df.select_dtypes(include=[np.number]).columns.drop('label', errors='ignore')
corr_matrix = df[col_numericas].corr()

threshold = 0.7
edges = []
for i in range(len(corr_matrix.columns)):
    for j in range(i + 1, len(corr_matrix.columns)):
        var1 = corr_matrix.columns[i]
        var2 = corr_matrix.columns[j]
        corr_value = corr_matrix.iloc[i, j]
        if abs(corr_value) > threshold:
            edges.append((var1, var2, {'correlation': corr_value}))

# ==========================================================
# Crear grafo y posiciones
# ==========================================================
G = nx.Graph()
G.add_nodes_from(corr_matrix.columns)
G.add_edges_from(edges)

pos = nx.spring_layout(G, k=1.0, iterations=50, seed=42)  # mayor separaci√≥n

# ==========================================================
# Preparar trazos para aristas
# ==========================================================
pos_edges_x, pos_edges_y, neg_edges_x, neg_edges_y = [], [], [], []

for edge in G.edges():
    x0, y0 = pos[edge[0]]
    x1, y1 = pos[edge[1]]
    corr = G.get_edge_data(edge[0], edge[1])['correlation']
    if corr > 0:
        pos_edges_x.extend([x0, x1, None])
        pos_edges_y.extend([y0, y1, None])
    else:
        neg_edges_x.extend([x0, x1, None])
        neg_edges_y.extend([y0, y1, None])

pos_edge_trace = go.Scatter(
    x=pos_edges_x, y=pos_edges_y,
    line=dict(width=1.5, color='rgba(255,0,0,0.7)'),
    mode='lines',
    hoverinfo='none',
    name='Correlaci√≥n Positiva'
)
neg_edge_trace = go.Scatter(
    x=neg_edges_x, y=neg_edges_y,
    line=dict(width=1.5, color='rgba(0,0,255,0.7)'),
    mode='lines',
    hoverinfo='none',
    name='Correlaci√≥n Negativa'
)

# ==========================================================
# Preparar nodos (texto flotante)
# ==========================================================
node_x = [pos[node][0] for node in G.nodes()]
node_y = [pos[node][1] for node in G.nodes()]
node_text = []
for node, neighbors in G.adjacency():
    correlations = []
    for neighbor in neighbors:
        corr_val = G.get_edge_data(node, neighbor)['correlation']
        correlations.append(f"{neighbor}: {corr_val:.2f}")
    hover_text = f"<b>{node}</b><br>Correlaciones fuertes:<br>" + "<br>".join(correlations)
    node_text.append(hover_text)

node_trace = go.Scatter(
    x=node_x, y=node_y,
    mode='markers+text',
    text=[node for node in G.nodes()],     # solo el nombre visible sobre el nodo
    hovertext=node_text,                   # informaci√≥n detallada al pasar el cursor
    hoverinfo='text',
    textposition='top center',
    marker=dict(
        size=10,
        color='DarkBlue',
        line=dict(color='black', width=1)
    ),
    textfont=dict(size=10, color='black'),
    name='Variables'
)

# ==========================================================
# Layout final con marco
# ==========================================================
fig = go.Figure(
    data=[pos_edge_trace, neg_edge_trace, node_trace],
    layout=go.Layout(
        title=dict(text='', font=dict(size=16)),
        showlegend=True,
        hovermode='closest',
        margin=dict(b=20, l=5, r=5, t=40),
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        paper_bgcolor="white",
        plot_bgcolor="white",
        shapes=[
            dict(
                type="rect",
                xref="paper",
                yref="paper",
                x0=0, y0=0,
                x1=1, y1=1,
                line=dict(color="black", width=2),
                fillcolor="white",
                layer="below"
            )
        ]
    )
)

st.plotly_chart(fig, use_container_width=True)

# ==========================================================
# ==================== Preparaci√≥n de Datos ====================
# ==========================================================
st.markdown("""
## üõ†Ô∏è Preparaci√≥n de Datos para Modelamiento

Despu√©s de explorar y analizar el dataset, es necesario preparar los datos antes de aplicar modelos de machine learning.  
Esto incluye:

1. Imputaci√≥n de valores faltantes.  
2. Escalado de las variables num√©ricas.  
3. Codificaci√≥n de la variable target y partici√≥n Train/Test.
""")

# --------------------------
# 4.1 Imputaci√≥n (KNNImputer)
# --------------------------
st.markdown("### 4.1 Imputaci√≥n de Valores Faltantes (KNNImputer)")

# Seleccionar features num√©ricas (excluir Time y label si no son num√©ricas)
feature_cols = [col for col in df.columns if col != 'label' and df[col].dtype in [np.float64, np.int64]]
X = df[feature_cols]
y = df['label']

# Imputaci√≥n
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=feature_cols)

st.write("‚úÖ Imputaci√≥n completada. Se utiliz√≥ KNNImputer considerando correlaci√≥n entre sensores.")
st.write("% NA despu√©s de imputaci√≥n:", round(X_imputed.isna().sum().sum()/X_imputed.size*100, 2), "%")

# --------------------------
# 4.2 Escalado (StandardScaler)
# --------------------------
st.markdown("### 4.2 Escalado de Variables Num√©ricas (StandardScaler)")

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=feature_cols)

st.write("‚úÖ Escalado completado. Las variables num√©ricas ahora tienen media=0 y desviaci√≥n est√°ndar=1.")

# --------------------------
# 4.3 Codificaci√≥n y Train/Test Split
# --------------------------
st.markdown("### 4.3 Codificaci√≥n de Etiqueta y Divisi√≥n Train/Test")

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42
)

st.write(f"‚úÖ Divisi√≥n completada: Train={X_train.shape}, Test={X_test.shape}")
st.write("La variable target ha sido codificada con LabelEncoder para su uso en modelos categ√≥ricos.")

